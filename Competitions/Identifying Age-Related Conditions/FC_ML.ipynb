{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "f1d8a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "8e46e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv('train.csv')\n",
    "dataset_df['EJ'] = dataset_df['EJ'].replace({'A':0, 'B':1})\n",
    "dataset_df = dataset_df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "502a331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算特征的均值、方差、标准差\n",
    "mean_values = dataset_df.mean()\n",
    "var_values = dataset_df.var()\n",
    "std_values = dataset_df.std()\n",
    "# 填充缺失值为特征的均值\n",
    "dataset_df = dataset_df.fillna(mean_values)\n",
    "max_values = dataset_df.max()\n",
    "min_values = dataset_df.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ab8a46fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'BC', 'BD ', 'BN', 'BP', 'BQ', 'BR',\n",
      "       'BZ', 'CC', 'CD ', 'CF', 'CR', 'CS', 'CW ', 'DA', 'DE', 'DF', 'DH',\n",
      "       'DI', 'DL', 'DN', 'DU', 'DV', 'DY', 'EB', 'EE', 'EG', 'EH', 'EJ', 'EL',\n",
      "       'EP', 'EU', 'FC', 'FD ', 'FE', 'FI', 'FL', 'FR', 'FS', 'GB', 'GE', 'GF',\n",
      "       'GH', 'GI', 'GL'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 分割特征和类别\n",
    "X = dataset_df.iloc[:, :-1]  # 特征\n",
    "y = dataset_df.iloc[:, -1]   # 类别\n",
    "# X = X.dropna()  # 删除包含缺失值的行\n",
    "y = y[X.index]  # 保持与特征对应的类别\n",
    "\n",
    "# 对非数值类型特征进行独热编码\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# 初始化特征选择器\n",
    "k = 50  # 选择前k个重要特征\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "\n",
    "# 特征选择\n",
    "X_selected = selector.fit_transform(X_encoded, y)\n",
    "\n",
    "# 获取选择的特征索引\n",
    "selected_feature_indices = selector.get_support(indices=True)\n",
    "\n",
    "# 获取选择的特征名称\n",
    "selected_features = X_encoded.columns[selected_feature_indices]\n",
    "# selected_features = selected_features[:-2].append(pd.Index(['EJ']))\n",
    "# 输出选择的特征\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "8baf8f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = dataset_df.loc[:, selected_features], dataset_df.iloc[:, -1]\n",
    "var_values = X_train.var()\n",
    "std_values = X_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "77c7f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X = np.array(X_train, dtype=np.float32)\n",
    "y = np.array(y_train, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "# 转换为 PyTorch 的 Tensor 数据类型\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "9f9fb931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.5722\n",
      "Epoch [2/200], Loss: 0.4687\n",
      "Epoch [3/200], Loss: 0.4467\n",
      "Epoch [4/200], Loss: 0.4591\n",
      "Epoch [5/200], Loss: 0.4532\n",
      "Epoch [6/200], Loss: 0.4594\n",
      "Epoch [7/200], Loss: 0.4555\n",
      "Epoch [8/200], Loss: 0.4602\n",
      "Epoch [9/200], Loss: 0.4380\n",
      "Epoch [10/200], Loss: 0.4310\n",
      "Epoch [11/200], Loss: 0.4202\n",
      "Epoch [12/200], Loss: 0.4017\n",
      "Epoch [13/200], Loss: 0.3797\n",
      "Epoch [14/200], Loss: 0.3451\n",
      "Epoch [15/200], Loss: 0.3095\n",
      "Epoch [16/200], Loss: 0.2811\n",
      "Epoch [17/200], Loss: 0.2587\n",
      "Epoch [18/200], Loss: 0.2457\n",
      "Epoch [19/200], Loss: 0.2319\n",
      "Epoch [20/200], Loss: 0.2266\n",
      "Epoch [21/200], Loss: 0.2174\n",
      "Epoch [22/200], Loss: 0.2135\n",
      "Epoch [23/200], Loss: 0.2073\n",
      "Epoch [24/200], Loss: 0.2101\n",
      "Epoch [25/200], Loss: 0.1963\n",
      "Epoch [26/200], Loss: 0.2043\n",
      "Epoch [27/200], Loss: 0.1980\n",
      "Epoch [28/200], Loss: 0.1890\n",
      "Epoch [29/200], Loss: 0.1893\n",
      "Epoch [30/200], Loss: 0.1891\n",
      "Epoch [31/200], Loss: 0.1855\n",
      "Epoch [32/200], Loss: 0.1803\n",
      "Epoch [33/200], Loss: 0.1869\n",
      "Epoch [34/200], Loss: 0.1753\n",
      "Epoch [35/200], Loss: 0.1739\n",
      "Epoch [36/200], Loss: 0.1898\n",
      "Epoch [37/200], Loss: 0.1794\n",
      "Epoch [38/200], Loss: 0.1680\n",
      "Epoch [39/200], Loss: 0.1773\n",
      "Epoch [40/200], Loss: 0.1653\n",
      "Epoch [41/200], Loss: 0.1672\n",
      "Epoch [42/200], Loss: 0.1680\n",
      "Epoch [43/200], Loss: 0.1654\n",
      "Epoch [44/200], Loss: 0.1594\n",
      "Epoch [45/200], Loss: 0.1560\n",
      "Epoch [46/200], Loss: 0.1546\n",
      "Epoch [47/200], Loss: 0.1589\n",
      "Epoch [48/200], Loss: 0.1617\n",
      "Epoch [49/200], Loss: 0.1498\n",
      "Epoch [50/200], Loss: 0.1448\n",
      "Epoch [51/200], Loss: 0.1684\n",
      "Epoch [52/200], Loss: 0.1547\n",
      "Epoch [53/200], Loss: 0.1407\n",
      "Epoch [54/200], Loss: 0.1416\n",
      "Epoch [55/200], Loss: 0.1495\n",
      "Epoch [56/200], Loss: 0.1361\n",
      "Epoch [57/200], Loss: 0.1505\n",
      "Epoch [58/200], Loss: 0.1337\n",
      "Epoch [59/200], Loss: 0.1329\n",
      "Epoch [60/200], Loss: 0.1309\n",
      "Epoch [61/200], Loss: 0.1279\n",
      "Epoch [62/200], Loss: 0.1307\n",
      "Epoch [63/200], Loss: 0.1254\n",
      "Epoch [64/200], Loss: 0.1228\n",
      "Epoch [65/200], Loss: 0.1236\n",
      "Epoch [66/200], Loss: 0.1209\n",
      "Epoch [67/200], Loss: 0.1235\n",
      "Epoch [68/200], Loss: 0.1304\n",
      "Epoch [69/200], Loss: 0.1188\n",
      "Epoch [70/200], Loss: 0.1156\n",
      "Epoch [71/200], Loss: 0.1243\n",
      "Epoch [72/200], Loss: 0.1202\n",
      "Epoch [73/200], Loss: 0.1153\n",
      "Epoch [74/200], Loss: 0.1125\n",
      "Epoch [75/200], Loss: 0.1108\n",
      "Epoch [76/200], Loss: 0.1184\n",
      "Epoch [77/200], Loss: 0.1073\n",
      "Epoch [78/200], Loss: 0.1195\n",
      "Epoch [79/200], Loss: 0.1055\n",
      "Epoch [80/200], Loss: 0.1069\n",
      "Epoch [81/200], Loss: 0.1038\n",
      "Epoch [82/200], Loss: 0.1025\n",
      "Epoch [83/200], Loss: 0.1136\n",
      "Epoch [84/200], Loss: 0.0996\n",
      "Epoch [85/200], Loss: 0.0983\n",
      "Epoch [86/200], Loss: 0.0974\n",
      "Epoch [87/200], Loss: 0.0963\n",
      "Epoch [88/200], Loss: 0.0942\n",
      "Epoch [89/200], Loss: 0.0945\n",
      "Epoch [90/200], Loss: 0.0914\n",
      "Epoch [91/200], Loss: 0.0930\n",
      "Epoch [92/200], Loss: 0.1018\n",
      "Epoch [93/200], Loss: 0.0898\n",
      "Epoch [94/200], Loss: 0.0921\n",
      "Epoch [95/200], Loss: 0.0876\n",
      "Epoch [96/200], Loss: 0.0854\n",
      "Epoch [97/200], Loss: 0.0830\n",
      "Epoch [98/200], Loss: 0.0949\n",
      "Epoch [99/200], Loss: 0.0822\n",
      "Epoch [100/200], Loss: 0.0809\n",
      "Epoch [101/200], Loss: 0.0799\n",
      "Epoch [102/200], Loss: 0.0803\n",
      "Epoch [103/200], Loss: 0.0790\n",
      "Epoch [104/200], Loss: 0.0782\n",
      "Epoch [105/200], Loss: 0.0777\n",
      "Epoch [106/200], Loss: 0.0765\n",
      "Epoch [107/200], Loss: 0.0748\n",
      "Epoch [108/200], Loss: 0.0741\n",
      "Epoch [109/200], Loss: 0.0729\n",
      "Epoch [110/200], Loss: 0.0727\n",
      "Epoch [111/200], Loss: 0.0852\n",
      "Epoch [112/200], Loss: 0.0717\n",
      "Epoch [113/200], Loss: 0.0711\n",
      "Epoch [114/200], Loss: 0.0840\n",
      "Epoch [115/200], Loss: 0.0706\n",
      "Epoch [116/200], Loss: 0.0710\n",
      "Epoch [117/200], Loss: 0.0695\n",
      "Epoch [118/200], Loss: 0.0707\n",
      "Epoch [119/200], Loss: 0.0690\n",
      "Epoch [120/200], Loss: 0.0690\n",
      "Epoch [121/200], Loss: 0.0686\n",
      "Epoch [122/200], Loss: 0.0678\n",
      "Epoch [123/200], Loss: 0.0677\n",
      "Epoch [124/200], Loss: 0.0671\n",
      "Epoch [125/200], Loss: 0.0797\n",
      "Epoch [126/200], Loss: 0.0670\n",
      "Epoch [127/200], Loss: 0.0654\n",
      "Epoch [128/200], Loss: 0.0648\n",
      "Epoch [129/200], Loss: 0.0643\n",
      "Epoch [130/200], Loss: 0.0762\n",
      "Epoch [131/200], Loss: 0.0755\n",
      "Epoch [132/200], Loss: 0.0615\n",
      "Epoch [133/200], Loss: 0.0608\n",
      "Epoch [134/200], Loss: 0.0606\n",
      "Epoch [135/200], Loss: 0.0600\n",
      "Epoch [136/200], Loss: 0.0594\n",
      "Epoch [137/200], Loss: 0.0594\n",
      "Epoch [138/200], Loss: 0.0587\n",
      "Epoch [139/200], Loss: 0.0586\n",
      "Epoch [140/200], Loss: 0.0582\n",
      "Epoch [141/200], Loss: 0.0582\n",
      "Epoch [142/200], Loss: 0.0580\n",
      "Epoch [143/200], Loss: 0.0578\n",
      "Epoch [144/200], Loss: 0.0578\n",
      "Epoch [145/200], Loss: 0.0579\n",
      "Epoch [146/200], Loss: 0.0576\n",
      "Epoch [147/200], Loss: 0.0573\n",
      "Epoch [148/200], Loss: 0.0569\n",
      "Epoch [149/200], Loss: 0.0700\n",
      "Epoch [150/200], Loss: 0.0570\n",
      "Epoch [151/200], Loss: 0.0568\n",
      "Epoch [152/200], Loss: 0.0565\n",
      "Epoch [153/200], Loss: 0.0564\n",
      "Epoch [154/200], Loss: 0.0563\n",
      "Epoch [155/200], Loss: 0.0692\n",
      "Epoch [156/200], Loss: 0.0561\n",
      "Epoch [157/200], Loss: 0.0560\n",
      "Epoch [158/200], Loss: 0.0560\n",
      "Epoch [159/200], Loss: 0.0560\n",
      "Epoch [160/200], Loss: 0.0559\n",
      "Epoch [161/200], Loss: 0.0558\n",
      "Epoch [162/200], Loss: 0.0558\n",
      "Epoch [163/200], Loss: 0.0556\n",
      "Epoch [164/200], Loss: 0.0557\n",
      "Epoch [165/200], Loss: 0.0555\n",
      "Epoch [166/200], Loss: 0.0556\n",
      "Epoch [167/200], Loss: 0.0555\n",
      "Epoch [168/200], Loss: 0.0556\n",
      "Epoch [169/200], Loss: 0.0554\n",
      "Epoch [170/200], Loss: 0.0552\n",
      "Epoch [171/200], Loss: 0.0554\n",
      "Epoch [172/200], Loss: 0.0552\n",
      "Epoch [173/200], Loss: 0.0680\n",
      "Epoch [174/200], Loss: 0.0553\n",
      "Epoch [175/200], Loss: 0.0681\n",
      "Epoch [176/200], Loss: 0.0551\n",
      "Epoch [177/200], Loss: 0.0677\n",
      "Epoch [178/200], Loss: 0.0676\n",
      "Epoch [179/200], Loss: 0.0550\n",
      "Epoch [180/200], Loss: 0.0674\n",
      "Epoch [181/200], Loss: 0.0548\n",
      "Epoch [182/200], Loss: 0.0676\n",
      "Epoch [183/200], Loss: 0.0548\n",
      "Epoch [184/200], Loss: 0.0548\n",
      "Epoch [185/200], Loss: 0.0548\n",
      "Epoch [186/200], Loss: 0.0547\n",
      "Epoch [187/200], Loss: 0.0548\n",
      "Epoch [188/200], Loss: 0.0546\n",
      "Epoch [189/200], Loss: 0.0673\n",
      "Epoch [190/200], Loss: 0.0545\n",
      "Epoch [191/200], Loss: 0.0545\n",
      "Epoch [192/200], Loss: 0.0545\n",
      "Epoch [193/200], Loss: 0.0544\n",
      "Epoch [194/200], Loss: 0.0544\n",
      "Epoch [195/200], Loss: 0.0544\n",
      "Epoch [196/200], Loss: 0.0544\n",
      "Epoch [197/200], Loss: 0.0545\n",
      "Epoch [198/200], Loss: 0.0545\n",
      "Epoch [199/200], Loss: 0.0544\n",
      "Epoch [200/200], Loss: 0.0543\n",
      "Test Accuracy: 0.8871\n"
     ]
    }
   ],
   "source": [
    "# 定义神经网络模型\n",
    "seed = 2\n",
    "torch.manual_seed(seed)\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        y = x\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x, y\n",
    "\n",
    "# 初始化神经网络\n",
    "net_model = NeuralNetwork()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net_model.parameters(), lr=0.001)\n",
    "\n",
    "# 将数据转换为 DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 训练神经网络\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    net_model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, layer = net_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 测试神经网络\n",
    "net_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred, layer = net_model(X_test)\n",
    "    y_pred_class = (y_pred >= 0.5).float()\n",
    "    accuracy = (y_pred_class == y_test).float().mean()\n",
    "    print(f\"Test Accuracy: {accuracy.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "acd77cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb = {\n",
    "    'objective': 'binary',  # 二分类目标函数\n",
    "    'metric': 'binary_logloss',  # 二分类损失函数\n",
    "    'boosting_type': 'gbdt',  # 提升类型，可选参数：'gbdt', 'dart', 'goss'\n",
    "    'num_leaves': 28,  # 叶子节点数量\n",
    "    'learning_rate': 0.1,  # 学习率\n",
    "    'feature_fraction': 0.85,  # 特征采样比例\n",
    "    'bagging_fraction': 0.7,  # 数据采样比例\n",
    "    'bagging_freq': 4,  # 数据采样频率\n",
    "    'random_state': 42,\n",
    "    'min_child_samples': 8,\n",
    "    'verbose':-1, \n",
    "    'random_state':12\n",
    "}\n",
    "params_cat = {\n",
    "    'iterations': 626,\n",
    "    'learning_rate': 0.01,\n",
    "    'depth': 4,\n",
    "    'l2_leaf_reg': 0.27,\n",
    "    'border_count': 66,\n",
    "    'random_state': 12,\n",
    "    'verbose': False\n",
    "}\n",
    "params_xgb = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.0356,\n",
    "    'max_depth': 7,\n",
    "    'subsample': 0.95,\n",
    "    'colsample_bytree': 0.92,\n",
    "    'gamma': 9e-08,\n",
    "    'random_state': 12\n",
    "}\n",
    "params_RF = {\n",
    "    'n_estimators': 300,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 3,\n",
    "    'max_features': 'auto',\n",
    "    'random_state': 12\n",
    "}\n",
    "params_net = {\n",
    "    'hidden_layer_sizes': (200,50), \n",
    "    'activation': 'relu', \n",
    "    'solver': 'adam', \n",
    "    'alpha': 0.004524225053160557, \n",
    "    'batch_size': 'auto', \n",
    "    'learning_rate': 'constant', \n",
    "    'learning_rate_init': 0.0032695885785495216, \n",
    "    'power_t': 0.41229208337868917, \n",
    "    'max_iter': 391, \n",
    "    'shuffle': True, \n",
    "    'random_state': None, \n",
    "    'tol': 0.0002505082147304683, \n",
    "    'verbose': False, \n",
    "    'warm_start': False, \n",
    "    'momentum': 0.8862529058691623, \n",
    "    'nesterovs_momentum': True, \n",
    "    'early_stopping': True, \n",
    "    'validation_fraction': 0.1250369526043429,      \n",
    "    'beta_1': 0.8337047263985674, \n",
    "    'beta_2': 0.9945669606589083, \n",
    "    'epsilon': 1.479694727954582e-09, \n",
    "    'n_iter_no_change': 19, \n",
    "    'max_fun': 16800\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8b16fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = dataset_df.loc[:, selected_features], dataset_df.iloc[:, -1]\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "\n",
    "# 转换为 PyTorch 的 Tensor 数据类型\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "aeeed4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_train = net_model(X_train)\n",
    "X_train = pd.DataFrame(X_train.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "796a8a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93       101\n",
      "           1       0.74      0.61      0.67        23\n",
      "\n",
      "    accuracy                           0.89       124\n",
      "   macro avg       0.83      0.78      0.80       124\n",
      "weighted avg       0.88      0.89      0.88       124\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99        98\n",
      "           1       1.00      0.96      0.98        26\n",
      "\n",
      "    accuracy                           0.99       124\n",
      "   macro avg       0.99      0.98      0.99       124\n",
      "weighted avg       0.99      0.99      0.99       124\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       107\n",
      "           1       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00       123\n",
      "   macro avg       1.00      1.00      1.00       123\n",
      "weighted avg       1.00      1.00      1.00       123\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       101\n",
      "           1       1.00      0.91      0.95        22\n",
      "\n",
      "    accuracy                           0.98       123\n",
      "   macro avg       0.99      0.95      0.97       123\n",
      "weighted avg       0.98      0.98      0.98       123\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.85, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.85\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       102\n",
      "           1       1.00      0.86      0.92        21\n",
      "\n",
      "    accuracy                           0.98       123\n",
      "   macro avg       0.99      0.93      0.95       123\n",
      "weighted avg       0.98      0.98      0.97       123\n",
      "\n",
      "1.0\n",
      "0.7171717171717172\n",
      "CPU times: total: 11.8 s\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Models = []\n",
    "best_acu = 0\n",
    "best_threshold = 0\n",
    "# 定义交叉验证的折数\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# 定义模型列表\n",
    "models = [\n",
    "    lgb.LGBMClassifier(**params_lgb),\n",
    "    #cb.CatBoostClassifier(**params_cat),\n",
    "    xgb.XGBClassifier(**params_xgb),\n",
    "    RandomForestClassifier(**params_RF),\n",
    "    # MLPClassifier(**params_net)\n",
    "    # SVC(probability=True, random_state=42)  # 设置 probability=True 以输出概率\n",
    "]\n",
    "\n",
    "for train_index, valid_index in kf.split(X_train):\n",
    "    \n",
    "    X_tr, X_val = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "    y_tr, y_val = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "    \n",
    "    # 训练模型\n",
    "    for model in models:\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "    # 预测结果的概率值\n",
    "    y_pred_proba_list = []\n",
    "    for model in models:\n",
    "#         if type(model).__name__ == 'MLPClassifier':\n",
    "#             scaler = StandardScaler()\n",
    "#             features_to_scale = X_val.columns\n",
    "#             X_val[features_to_scale] = scaler.fit_transform(X_val[features_to_scale])\n",
    "        y_pred_proba = model.predict_proba(X_val)[:, 1]  # 取第一列的概率值（正类的概率）\n",
    "        y_pred_proba_list.append(y_pred_proba)\n",
    "        \n",
    "#     for i in range(len(y_pred_proba_list)):\n",
    "#         thresholds = np.linspace(0, 1, 100)\n",
    "#         acu = 0\n",
    "#         thred = 0\n",
    "#         for threshold in thresholds:\n",
    "#             y_pred = np.where(y_pred_proba_list[i] >= threshold, 1, 0)\n",
    "#             accuracy = np.mean(y_pred == y_val)\n",
    "#             if accuracy > acu:\n",
    "#                 acu = accuracy\n",
    "#                 thred = threshold\n",
    "#                 print('acu=', acu)\n",
    "#         y_pred_proba_list[i] = [map_values(j, thred=thred) for j in y_pred_proba_list[i]]\n",
    "            \n",
    "\n",
    "    # 模型融合（平均概率值）\n",
    "    y_pred_ensemble_proba = np.mean(y_pred_proba_list, axis=0)\n",
    "    \n",
    "    #选取最佳阈值\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    temp_acu = 0\n",
    "    temp_thred = 0\n",
    "    for threshold in thresholds:\n",
    "        # 计算最佳阈值\n",
    "        y_pred_ensemble = np.where(y_pred_ensemble_proba >= threshold, 1, 0)\n",
    "        accuracy = np.mean(y_pred_ensemble == y_val)\n",
    "        if accuracy >= temp_acu:\n",
    "            temp_acu = accuracy\n",
    "            temp_thred = threshold\n",
    "            temp_y_pred = y_pred_ensemble\n",
    "    print(classification_report(y_val, temp_y_pred))\n",
    "    \n",
    "    # 保存最优模型和最优阈值\n",
    "    if temp_acu >= best_acu:\n",
    "        Models = models\n",
    "        best_acu = temp_acu\n",
    "        best_threshold = temp_thred\n",
    "print(best_acu)\n",
    "print(best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "9e1490cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test3.csv',index_col=\"Id\")\n",
    "# 填充缺失值为特征的均值\n",
    "for i in range(len(test_df['EJ'])):\n",
    "    if test_df['EJ'][i] != 'A' and test_df['EJ'][i] != 'B':\n",
    "        test_df['EJ'][i] = 'A'\n",
    "test_df['EJ'] = test_df['EJ'].replace({'A':0, 'B':1})\n",
    "test_df = test_df.fillna(mean_values)\n",
    "test_df.replace([np.inf], [np.nan], inplace=True)\n",
    "test_df = test_df.fillna(max_values)\n",
    "test_df.replace([-np.inf], [np.nan], inplace=True)\n",
    "test_df = test_df.fillna(min_values)\n",
    "test_df = test_df.loc[:, selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "4e69f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = dataset_df.loc[:, selected_features], dataset_df.iloc[:, -1]\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "test_df = scaler.transform(test_df)\n",
    "# test_df = scaler.transform(test_df)\n",
    "# test_df = (test_df - X_train.mean()) / X_train.var() + X_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ee4cf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = np.array(test_df, dtype=np.float32)\n",
    "\n",
    "# 转换为 PyTorch 的 Tensor 数据类型\n",
    "test_df = torch.tensor(test_df, dtype=torch.float32)\n",
    "net_model.eval()\n",
    "_, test_df = net_model(test_df)\n",
    "test_df = pd.DataFrame(test_df.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e8be1815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9909859436075742 1\n",
      "0.011915291943998181 0\n",
      "0.019583076357815414 0\n",
      "0.011915485125482501 0\n",
      "0.9192141076579627 1\n",
      "0.00901717936197727 0\n",
      "0.013694717241554384 0\n",
      "0.009018110652202241 0\n",
      "0.009297493872659962 0\n",
      "0.009017183957869754 0\n",
      "0.8894745471496269 1\n",
      "0.013960055875697958 0\n",
      "0.013694717241554384 0\n",
      "0.8756462064233949 1\n",
      "0.017592845718902714 0\n",
      "0.01679364197943872 0\n",
      "0.023941209159791598 0\n",
      "0.02590245371724774 0\n",
      "0.01985395778385414 0\n",
      "0.00929746211273547 0\n",
      "0.013694717241554384 0\n",
      "0.01443795474963539 0\n",
      "0.016714005842316335 0\n",
      "0.011915455646412249 0\n",
      "0.014594862958506141 0\n",
      "0.009017179659356288 0\n",
      "0.00901718393724817 0\n",
      "0.011915469516618843 0\n",
      "0.013269210785987718 0\n",
      "0.013694717241554384 0\n",
      "0.013694717241554384 0\n",
      "0.9868605575935661 1\n",
      "0.8833726952977751 1\n",
      "0.011771212946326602 0\n",
      "0.00901718393724817 0\n",
      "0.05464695504892151 0\n",
      "0.9906611101815955 1\n",
      "0.009017183950074099 0\n",
      "0.06213674677952521 0\n",
      "0.013694717241554384 0\n",
      "0.00901718393724817 0\n",
      "0.9909859635137422 1\n",
      "0.009319068859152521 0\n",
      "0.014486499949565154 0\n",
      "0.00901718393724817 0\n",
      "0.013356197530370748 0\n",
      "0.009018110656461973 0\n",
      "0.00929749387516038 0\n",
      "0.025691584044099658 0\n",
      "0.990985937530723 1\n",
      "0.009297493873732826 0\n",
      "0.013694717241554384 0\n",
      "0.026853383496580858 0\n",
      "0.009017183946782697 0\n",
      "0.00901718395588684 0\n",
      "0.01690299837603919 0\n",
      "0.009018110652202241 0\n",
      "0.009319068859152521 0\n",
      "0.00929748937580339 0\n",
      "0.00901718394987026 0\n",
      "0.05500161771278858 0\n",
      "0.05515284962391807 0\n",
      "0.01705867362518379 0\n",
      "0.011771212946326602 0\n",
      "0.014487040213206704 0\n",
      "0.01500156107491233 0\n",
      "0.00901718395588684 0\n",
      "0.029221747692716047 0\n",
      "0.01447537321757085 0\n",
      "0.015719060978331558 0\n",
      "0.013694717241554384 0\n",
      "0.01595761670962205 0\n",
      "0.28331408500033217 0\n",
      "0.037394625087612175 0\n",
      "0.009017183946782697 0\n",
      "0.9909859436075742 1\n",
      "0.9909859409303842 1\n",
      "0.009017183957869754 0\n",
      "0.01707135138399996 0\n",
      "0.9909859436075742 1\n",
      "0.009017183957869754 0\n",
      "0.009017183957869754 0\n",
      "0.055511390578154886 0\n",
      "0.014486516593982904 0\n",
      "0.009543945468213466 0\n",
      "0.009017183957869754 0\n",
      "0.021003112301288606 0\n",
      "0.0230219509849387 0\n",
      "0.9894083246403315 1\n",
      "0.011915291943998181 0\n",
      "0.01191744775491657 0\n",
      "0.990985937530723 1\n",
      "0.0621790917333517 0\n",
      "0.9868859873109891 1\n",
      "0.00929749388914171 0\n",
      "0.9901038157850373 1\n",
      "0.9909859508424615 1\n",
      "0.014437969527496541 0\n",
      "0.009297493903729174 0\n",
      "0.011915469731970182 0\n",
      "0.00929749389084364 0\n",
      "0.014486516593982904 0\n",
      "0.7429325970995914 0\n",
      "0.04442374252054764 0\n",
      "0.009017183957869754 0\n",
      "0.013694717241554384 0\n",
      "0.009319068859152521 0\n",
      "0.009544399007148591 0\n",
      "0.009986933949301046 0\n",
      "0.990986905446278 1\n",
      "0.013694717241554384 0\n",
      "0.016954617233257304 0\n",
      "0.009017183957869754 0\n",
      "0.009018110652202241 0\n",
      "0.014486499949565154 0\n",
      "0.009017183951161972 0\n",
      "0.011771212946326602 0\n",
      "0.009319068859152521 0\n",
      "0.990985915874698 1\n",
      "0.017071375684839452 0\n",
      "0.009297462068731168 0\n",
      "0.018501733718953518 0\n",
      "0.027167155334669263 0\n",
      "0.009319068859152521 0\n",
      "0.013694633619067725 0\n",
      "0.009017183949220733 0\n",
      "0.01679364197943872 0\n",
      "0.990985937530723 1\n",
      "0.010777296418737802 0\n",
      "0.016913034976468965 0\n",
      "0.00901718393724817 0\n",
      "0.990986905446278 1\n",
      "0.014437979042576978 0\n",
      "0.048569833453174865 0\n",
      "0.9909829355539556 1\n",
      "0.05775446104311597 0\n",
      "0.009017179659356288 0\n",
      "0.024451490467604742 0\n",
      "0.009017183957869754 0\n",
      "0.047548668594681674 0\n",
      "0.03469039646927327 0\n",
      "0.009018110652202241 0\n",
      "0.013694717241554384 0\n",
      "0.009017183957869754 0\n",
      "0.011771212946326602 0\n",
      "0.4872812423239181 1\n",
      "0.9909859635137422 1\n",
      "0.010468298939583254 0\n",
      "0.02117710570817061 0\n",
      "0.009319068827392314 0\n",
      "0.677739308968783 1\n",
      "0.011915469731970182 0\n",
      "0.05562647787889821 0\n",
      "0.011917543929570383 0\n",
      "0.009018110652202241 0\n",
      "0.677739308968783 1\n",
      "0.009319068790804633 0\n",
      "0.057401644537712744 0\n",
      "0.20796975351349234 0\n",
      "0.009297493872659962 0\n",
      "0.677739308968783 1\n",
      "0.02071841760077774 0\n",
      "0.99098781357175 1\n",
      "0.013958765626956521 0\n",
      "0.050888427875290854 0\n",
      "0.9909859434514846 1\n",
      "0.009017183951161972 0\n",
      "0.01674815405034599 0\n",
      "0.016067953466234863 0\n",
      "0.011915469731970182 0\n",
      "0.009017179659356288 0\n",
      "0.016531638090003412 0\n",
      "0.009297493888045835 0\n",
      "0.022141392833883982 0\n",
      "0.019494125004189126 0\n",
      "0.011030444953728935 0\n",
      "0.009017183946782697 0\n",
      "0.013694717241554384 0\n",
      "0.016793634530631176 0\n",
      "0.00901718393724817 0\n",
      "0.00901718395588684 0\n",
      "0.9879290360425518 1\n",
      "0.00931906883138072 0\n",
      "0.009017183957869754 0\n",
      "0.009544399588310232 0\n",
      "0.06340106834574279 0\n",
      "0.9909859507740523 1\n",
      "0.009543945962987199 0\n",
      "0.009544395579074511 0\n",
      "0.990986905446278 1\n",
      "0.016073115446261863 0\n",
      "0.013694717241554384 0\n",
      "0.009297493872659962 0\n",
      "0.9900956284115342 1\n",
      "0.023160103398628062 0\n",
      "0.016795887074527147 0\n",
      "0.016883370951444238 0\n",
      "0.009543972450057035 0\n",
      "0.05563005430025558 0\n",
      "0.021215383985122196 0\n",
      "0.025065417604961924 0\n",
      "0.011915469731970182 0\n",
      "0.014835170702362155 0\n",
      "0.799133873673525 0\n",
      "0.009657081686728729 0\n",
      "0.011771212946326602 0\n",
      "0.04803184321673595 0\n",
      "0.009319068841160304 0\n",
      "0.009019253436542484 0\n",
      "0.05745439687804885 0\n",
      "0.009319068859152521 0\n",
      "0.00901718395588684 0\n",
      "0.009017183957869754 0\n",
      "0.00929749388914171 0\n",
      "0.009544345298900024 0\n",
      "0.1525001432483953 0\n",
      "0.014475373578423038 0\n",
      "0.016883542635547977 0\n",
      "0.9909859434514846 1\n",
      "0.009017183957869754 0\n",
      "0.014486513206048407 0\n",
      "0.009297470294290289 0\n",
      "0.00901718393724817 0\n",
      "0.01191545549848063 0\n",
      "0.02059292977979428 0\n",
      "0.009018110656461973 0\n",
      "0.014437979073166454 0\n",
      "0.009297493903729174 0\n",
      "0.013696173544805753 0\n",
      "0.9043125410691609 1\n",
      "0.013694717241554384 0\n",
      "0.014437981028447677 0\n",
      "0.9909859508424615 1\n",
      "0.009017183957869754 0\n",
      "0.011915479610861058 0\n",
      "0.00901718393724817 0\n",
      "0.013694717241554384 0\n",
      "0.014840468507256904 0\n",
      "0.397852500928024 1\n",
      "0.009017183957869754 0\n",
      "0.011771212946326602 0\n",
      "0.00901718395588684 0\n",
      "0.016793632104922084 0\n",
      "0.00929749387516038 0\n",
      "0.014438885268570878 0\n",
      "0.014437964189302085 0\n",
      "0.01679364197943872 0\n",
      "0.009017179659356288 0\n",
      "0.03823093939431176 0\n",
      "0.9251195488904602 1\n",
      "0.06198879832272042 0\n",
      "0.016255258478162896 0\n",
      "0.011915291943998181 0\n",
      "0.00901718393724817 0\n",
      "0.016882694550287913 0\n",
      "0.009987162375772178 0\n",
      "0.7437088916490536 1\n",
      "0.020330434275955905 0\n",
      "0.9909829355539556 1\n",
      "0.9909805421099943 1\n",
      "0.009017183957869754 0\n",
      "0.990986905446278 1\n",
      "0.9909805421099943 1\n",
      "0.013694717241554384 0\n",
      "0.01679364197943872 0\n",
      "0.9909859409303842 1\n",
      "0.009017183957869754 0\n",
      "0.40135016125289164 1\n",
      "0.9909859635137422 1\n",
      "0.00901718394987026 0\n",
      "0.011915291943998181 0\n",
      "0.05293954653790486 0\n",
      "0.014840252302007818 0\n",
      "0.00901718393724817 0\n",
      "0.9909859434581604 1\n",
      "0.9909859436075742 1\n",
      "0.014840114494284441 0\n",
      "0.05346392226707722 0\n",
      "0.016954477228827917 0\n",
      "0.056318179329685164 0\n",
      "0.026057999125479334 0\n",
      "0.01448705075732335 0\n",
      "0.9909859436075742 1\n",
      "0.014486510242039693 0\n",
      "0.023809449449401356 0\n",
      "0.01714173555076692 0\n",
      "0.013694717241554384 0\n",
      "0.009017183957869754 0\n",
      "0.9909805421099943 1\n",
      "0.013694717241554384 0\n",
      "0.9906611210616357 1\n",
      "0.009017183957869754 0\n",
      "0.009017183957869754 0\n",
      "0.011771212946326602 0\n",
      "0.9896319671730222 1\n",
      "0.024035367968747334 0\n",
      "0.00929749388914171 0\n",
      "0.01444185756743232 0\n",
      "0.009319068813388691 0\n",
      "0.0173234021415536 0\n",
      "0.9909805421099943 1\n",
      "0.990985937530723 1\n",
      "0.017436704397616486 0\n",
      "0.026336618659460766 0\n",
      "0.9909859434514846 1\n",
      "0.00901718393724817 0\n",
      "0.009017183951161972 0\n",
      "0.9909829355539556 1\n",
      "0.015217165647182127 0\n",
      "0.011915485454964361 0\n",
      "0.011771212946326602 0\n",
      "0.009319068859152521 0\n",
      "0.009319068808796505 0\n",
      "0.009017183946782697 1\n",
      "0.014838067231508241 0\n",
      "0.009319068859152521 0\n",
      "0.00929749388914171 0\n",
      "0.009319068859152521 0\n",
      "0.3381066568391308 1\n",
      "0.009319068859152521 0\n",
      "0.016793634530631176 0\n",
      "0.009297493871700282 0\n",
      "0.014437981028447677 0\n",
      "0.018801127480530398 0\n",
      "0.9909829355539556 1\n",
      "0.009319068859152521 0\n",
      "0.015949734386206223 0\n",
      "0.04143523470053188 0\n",
      "0.05172344471030769 0\n",
      "0.025440262255741575 0\n",
      "0.8906999983105202 1\n",
      "0.021245876381826827 0\n",
      "0.019895964647627024 0\n",
      "0.01653977762558267 0\n",
      "0.013694717241554384 0\n",
      "0.053659022463424244 0\n",
      "0.1902157596118854 0\n",
      "0.013694717241554384 0\n",
      "0.018146015774318864 0\n",
      "0.00929749388914171 0\n",
      "0.15477183414118126 0\n",
      "0.01679360087555987 0\n",
      "0.05759675897298062 0\n",
      "0.009018110656461973 0\n",
      "0.9909864100306255 1\n",
      "0.014052276438314401 0\n",
      "0.009017179659356288 0\n",
      "0.014437978985525216 0\n",
      "0.9909859434514846 1\n",
      "0.02207513048207703 0\n",
      "0.009297788862708215 0\n",
      "0.009319068859152521 0\n",
      "0.013694717241554384 0\n",
      "0.014840468507256904 0\n",
      "0.009989565508118465 0\n",
      "0.009319068859137533 0\n",
      "0.00901718393724817 0\n",
      "0.9909859436075742 1\n",
      "0.00901718393724817 0\n",
      "0.009544399007148591 0\n",
      "0.01558724633925168 0\n",
      "0.009017183951161972 0\n",
      "0.01443792185527793 0\n",
      "0.014487747375375448 0\n",
      "0.009319068859152521 0\n",
      "0.02584841239715723 0\n",
      "0.019416806101535577 0\n",
      "0.015217165647182127 0\n",
      "0.8877123439895325 1\n",
      "0.045236017887592184 0\n",
      "0.014487040213206704 0\n",
      "0.9909859436075742 1\n",
      "0.015217165647182127 0\n",
      "0.009543972427472977 0\n",
      "0.00901718393724817 0\n",
      "0.15370257083325653 0\n",
      "0.014437978985525216 0\n",
      "0.009017179659356288 0\n",
      "0.9036273558839758 1\n",
      "0.00901718393724817 0\n",
      "0.01760954287494408 0\n",
      "0.011771212946326602 0\n",
      "0.009017183957869754 0\n",
      "0.00929748937580339 0\n",
      "0.009017183957869754 0\n",
      "0.014486516593982904 0\n",
      "0.013694717241554384 0\n",
      "0.9909859507740523 1\n",
      "0.014839637099513548 0\n",
      "0.009297493872659962 0\n",
      "0.04525126241947114 0\n",
      "0.00929746211273547 1\n",
      "0.01443795474963539 0\n",
      "0.011915485125482501 0\n",
      "0.013694717241554384 0\n",
      "0.01444173924184324 0\n",
      "0.00929749388914171 0\n",
      "0.014486516593982904 0\n",
      "0.011822892532889909 0\n",
      "0.00931906885985973 0\n",
      "0.026186760743897645 0\n",
      "0.013694717241554384 0\n",
      "0.02863543870687967 0\n",
      "0.00901718393724817 0\n",
      "0.04080262880365101 0\n",
      "0.00929748937580339 0\n",
      "0.8756462064233949 1\n",
      "0.014437979030952291 0\n",
      "0.9909867916712151 1\n",
      "0.014840468507256904 0\n",
      "0.06089140558483249 0\n",
      "0.03246047122795069 0\n",
      "0.3919454664834312 1\n",
      "0.9909864100306255 1\n",
      "0.00901718395588684 0\n",
      "0.9909859436075742 1\n",
      "0.017670627692190275 0\n",
      "0.014437981028447677 0\n",
      "0.00901718393724817 0\n",
      "0.01666845480733804 0\n",
      "0.009297481058472784 0\n",
      "0.9902888577088117 1\n",
      "0.050159199379816166 0\n",
      "0.9909859434581604 1\n",
      "0.03357401193464677 0\n",
      "0.9909859436075742 1\n",
      "0.009017183957869754 0\n",
      "0.01679360087555987 0\n",
      "0.011565319650827735 0\n",
      "0.014441736141597878 0\n",
      "0.01848072715734873 0\n",
      "0.00901718395588684 0\n",
      "0.009319068845133742 0\n",
      "0.919944602113708 1\n",
      "0.990985937530723 1\n",
      "0.009017183957869754 0\n",
      "0.9909862222021072 1\n",
      "0.00901718393724817 0\n",
      "0.011771212946326602 0\n",
      "0.014437981028447677 0\n",
      "0.009017183949220733 0\n",
      "0.018866598577510663 0\n",
      "0.011771212946326602 0\n",
      "0.009297493871700282 0\n",
      "0.013694717241554384 0\n",
      "0.043674762044043434 0\n",
      "0.00929749388914171 0\n",
      "0.014486499949565154 0\n",
      "0.009319068859152521 0\n",
      "0.014437981028447677 0\n",
      "0.00901718394987026 0\n",
      "0.009017183946782697 0\n",
      "0.009392434191500407 0\n",
      "0.01601046805079052 0\n",
      "0.01191525645182511 0\n",
      "0.01760954287494408 0\n",
      "0.0173397582955659 0\n",
      "0.015278807306998715 0\n",
      "0.03029625435435657 0\n",
      "0.05443062740169585 0\n",
      "0.009017183951161972 0\n",
      "0.018319971870747395 0\n",
      "0.014840329885276492 0\n",
      "0.009297489299810075 0\n",
      "0.009319068841160304 0\n",
      "0.009017183951161972 0\n",
      "0.015216875968881308 0\n",
      "0.3904617237453743 1\n",
      "0.014840468507256904 0\n",
      "0.01737587321638777 0\n",
      "0.9121396253002233 1\n",
      "0.990985937530723 1\n",
      "0.009297493871700282 0\n",
      "0.013694717241554384 0\n",
      "0.00901718393724817 0\n",
      "0.05825732163319186 0\n",
      "0.009297470294290289 0\n",
      "0.017290101957695318 0\n",
      "0.009017183957869754 0\n",
      "0.9902888577088117 1\n",
      "0.009297493872659962 0\n",
      "0.02741021883089423 0\n",
      "0.009017175706052235 0\n",
      "0.011820438611926111 0\n",
      "0.00901718394987026 0\n",
      "0.009297793186639073 0\n",
      "0.4758109959304555 1\n",
      "0.9901984480971215 1\n",
      "0.011915291943998181 0\n",
      "0.014441864082165773 0\n",
      "0.7449624100221112 1\n",
      "0.009297462068731168 0\n",
      "0.009017183957869754 0\n",
      "0.9909859409303842 1\n",
      "0.9909864100306255 1\n",
      "0.00929749387516038 0\n",
      "0.028459727915963493 0\n",
      "0.046419573840230995 0\n",
      "0.009017183957869754 0\n",
      "0.009319068859152521 0\n",
      "0.9909859409303842 1\n",
      "0.19515403121682362 0\n",
      "0.011915412338936877 0\n",
      "0.014839637099513548 0\n",
      "0.009018110656461973 0\n",
      "0.019126092903586477 0\n",
      "0.009017183951161972 0\n",
      "0.00901718395588684 0\n",
      "0.013694717241554384 0\n",
      "0.3885563715246101 1\n",
      "0.00929749388914171 0\n",
      "0.9121396253002233 1\n",
      "0.01261978311756487 0\n",
      "0.009017183957869754 0\n",
      "0.014835170702362155 0\n",
      "0.009017183953161664 0\n",
      "0.9909762258748994 1\n",
      "0.016793610775056745 0\n",
      "0.8756462064233949 1\n",
      "0.014836600878839921 0\n",
      "0.00901718393724817 0\n",
      "0.01036203503665203 0\n",
      "0.014951475471414663 0\n",
      "0.014840468507256904 0\n",
      "0.01831873506364351 0\n",
      "0.009319068859152521 0\n",
      "0.9152881817320369 1\n",
      "0.014840468507256904 0\n",
      "0.00901718394987026 0\n",
      "0.014486499949565154 0\n",
      "0.9909859436075742 1\n",
      "0.014783546421501037 0\n",
      "0.00901718393724817 0\n",
      "0.015509291678648236 0\n",
      "0.009297785013549281 0\n",
      "0.014486499949565154 0\n",
      "0.990985915874698 1\n",
      "0.01595761670962205 0\n",
      "0.9909864100306255 1\n",
      "0.013694717241554384 0\n",
      "0.011771212946326602 0\n",
      "0.009018110652202241 0\n",
      "0.019448373138533204 0\n",
      "0.013694717241554384 0\n",
      "0.009017179659356288 0\n",
      "0.013694717241554384 0\n",
      "0.8786338607443825 1\n",
      "0.011915485125482501 0\n",
      "0.9909859507740523 1\n",
      "0.009319068862172994 0\n",
      "0.06213686035658852 0\n",
      "0.012837947033040031 0\n",
      "0.014638044921671334 0\n",
      "0.1902157596118854 0\n",
      "0.013227021934442302 0\n",
      "0.9909859434514846 1\n",
      "0.01679364197943872 0\n",
      "0.009544398887511645 0\n",
      "0.020905393154750887 0\n",
      "0.9909859434514846 1\n",
      "0.009377479922462602 0\n",
      "0.012303241377728806 0\n",
      "0.009913798050078262 0\n",
      "0.9909859434581604 1\n",
      "0.990985937530723 1\n",
      "0.011771212946326602 0\n",
      "0.01595761670962205 0\n",
      "0.06304569289908767 0\n",
      "0.009017183946782697 0\n",
      "0.009017183957869754 0\n",
      "0.045638416522716135 0\n",
      "0.014437978985525216 0\n",
      "0.011915469342206761 0\n",
      "0.009017183946782697 0\n",
      "0.011915485125482501 0\n",
      "0.009297493871700282 0\n",
      "0.7546398923909504 1\n",
      "0.009017183953161664 0\n",
      "0.01676911430337433 0\n",
      "0.009297493872659962 0\n",
      "0.677739308968783 0\n",
      "0.00901718393724817 0\n",
      "0.009017183946782697 0\n",
      "0.016715127386295006 0\n",
      "0.009297493886121238 0\n",
      "0.9909762258748994 1\n",
      "0.017334748228001445 0\n",
      "0.009017179659356288 0\n",
      "0.9240456773228657 1\n",
      "0.009939125030987829 0\n",
      "0.00901718395588684 0\n",
      "0.00929749388914171 0\n",
      "0.7211850639729196 1\n",
      "0.011915455672693103 0\n",
      "0.009543972450057035 0\n",
      "0.00954432780598856 0\n",
      "0.009544399920838102 0\n",
      "0.009297493871700282 0\n",
      "0.009017183951161972 0\n",
      "0.009297493874087515 0\n",
      "0.011771212946326602 0\n",
      "0.009319068790804633 0\n",
      "0.019126092903586477 0\n",
      "0.01679574707009776 0\n",
      "0.009297492800745285 0\n",
      "0.00901718395588684 0\n",
      "0.03214148339871568 0\n",
      "0.012356844178330202 0\n",
      "0.9909859635137422 1\n",
      "0.009017183946782697 0\n",
      "0.016883401963963575 0\n",
      "0.009297493871700282 0\n",
      "0.014541774760700897 0\n",
      "0.04649024498457759 0\n",
      "0.009543945468213466 0\n",
      "0.013694717241554384 0\n",
      "0.00901718394987026 0\n"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "for model in Models:\n",
    "    y_pred_proba = model.predict_proba(test_df)[:, 1]  # 取第一列的概率值（正类的概率）\n",
    "    y_pred.append(y_pred_proba)\n",
    "\n",
    "# 模型融合（平均概率值）\n",
    "y_pred = np.mean(y_pred, axis=0)\n",
    "for i in range(len(y_pred)):\n",
    "    print(y_pred[i], y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "06696246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50456333 0.49543667]\n",
      " [0.9942084  0.0057916 ]\n",
      " [0.99048137 0.00951863]\n",
      " ...\n",
      " [0.99536103 0.00463897]\n",
      " [0.99334349 0.00665651]\n",
      " [0.99561707 0.00438293]]\n"
     ]
    }
   ],
   "source": [
    "def map_values(value,thred=best_threshold):\n",
    "    \"\"\"根据阈值映射\"\"\"\n",
    "    if value <= thred:\n",
    "        return 0.5 * (value / thred)\n",
    "    else:\n",
    "        return 0.5 + 0.5 * ((value - thred) / 0.65)\n",
    "y_pred = np.vectorize(map_values)(y_pred)\n",
    "pred = np.stack((1-y_pred, y_pred), axis=1)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "12781dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test3.csv')\n",
    "submission = pd.DataFrame(test[\"Id\"], columns = [\"Id\"]);\n",
    "submission[\"class_0\"] = 1 - y_pred\n",
    "submission[\"class_1\"] = y_pred\n",
    "\n",
    "submission.to_csv('submission.csv', index = None);\n",
    "submission_df = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "7c8e04cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000ff2bfdfe9</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.990986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>007255e47698</td>\n",
       "      <td>0.988085</td>\n",
       "      <td>0.011915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>013f2bd269f5</td>\n",
       "      <td>0.980417</td>\n",
       "      <td>0.019583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>043ac50845d5</td>\n",
       "      <td>0.988085</td>\n",
       "      <td>0.011915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>044fb8a146ec</td>\n",
       "      <td>0.080786</td>\n",
       "      <td>0.919214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>fd3dafe738fd</td>\n",
       "      <td>0.985458</td>\n",
       "      <td>0.014542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>fd895603f071</td>\n",
       "      <td>0.953510</td>\n",
       "      <td>0.046490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>fd8ef6377f76</td>\n",
       "      <td>0.990456</td>\n",
       "      <td>0.009544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>fe1942975e40</td>\n",
       "      <td>0.986305</td>\n",
       "      <td>0.013695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>ffcca4ded3bb</td>\n",
       "      <td>0.990983</td>\n",
       "      <td>0.009017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>617 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Id   class_0   class_1\n",
       "0    000ff2bfdfe9  0.009014  0.990986\n",
       "1    007255e47698  0.988085  0.011915\n",
       "2    013f2bd269f5  0.980417  0.019583\n",
       "3    043ac50845d5  0.988085  0.011915\n",
       "4    044fb8a146ec  0.080786  0.919214\n",
       "..            ...       ...       ...\n",
       "612  fd3dafe738fd  0.985458  0.014542\n",
       "613  fd895603f071  0.953510  0.046490\n",
       "614  fd8ef6377f76  0.990456  0.009544\n",
       "615  fe1942975e40  0.986305  0.013695\n",
       "616  ffcca4ded3bb  0.990983  0.009017\n",
       "\n",
       "[617 rows x 3 columns]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9152174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
