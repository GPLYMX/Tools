{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17dbd689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff10df38",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mChannelWise\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_features: \u001b[38;5;28mint\u001b[39m, out_features: \u001b[38;5;28mint\u001b[39m, bias: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      3\u001b[0m                  device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m         factory_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: device, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m: dtype}\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mChannelWise\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m         bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(fan_in) \u001b[38;5;28;01mif\u001b[39;00m fan_in \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     23\u001b[0m         init\u001b[38;5;241m.\u001b[39muniform_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;241m-\u001b[39mbound, bound)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: \u001b[43mTensor\u001b[49m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextra_repr\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tensor' is not defined"
     ]
    }
   ],
   "source": [
    "class ChannelWise(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\n",
    "        # https://github.com/pytorch/pytorch/issues/57109\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d4df616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelWise(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = False, device=None, dtype=None,\n",
    "                 input_split_size: int = 1):\n",
    "        super(ChannelWise, self).__init__(in_features, out_features, bias, device=None, dtype=None)\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.input_split_size = input_split_size\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_tensor = torch.chunk(input, self.input_split_size, dim=1)\n",
    "\n",
    "        length = len(input_tensor)\n",
    "        split_input_size = math.ceil(self.in_features / length)\n",
    "        split_output_size = math.ceil(self.out_features / length)\n",
    "\n",
    "        output_tensor = []\n",
    "        for i in range(length):\n",
    "            weight = Parameter(torch.empty((split_output_size, split_input_size), **self.factory_kwargs))\n",
    "            bias = Parameter(torch.empty(split_output_size, **self.factory_kwargs))\n",
    "            output_tensor.append(F.linear(input_tensor[i], weight, bias))\n",
    "        return torch.cat(output_tensor, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b08c08f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.linear.Linear'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ChannelWiseLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        # 继承父类的初始化函数\n",
    "        super(ChannelWiseLinear, self).__init__()\n",
    "        \n",
    "        # 输入特征数\n",
    "        self.in_features = in_features\n",
    "        # 输出特征数\n",
    "        self.out_features = out_features\n",
    "        # 是否使用偏置项\n",
    "        self.bias = bias\n",
    "        \n",
    "        # 创建可学习的权重参数\n",
    "        self.weights = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        # 创建可学习的偏置项参数（如果使用）\n",
    "        if bias:\n",
    "            self.biases = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('biases', None)\n",
    "        \n",
    "        # 初始化参数\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # 使用 Kaiming 初始化权重参数\n",
    "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5))\n",
    "        # 如果使用偏置项，则使用均匀分布初始化偏置项参数\n",
    "        if self.biases is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.biases, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # 创建与输入大小相同的零张量\n",
    "        output = torch.zeros_like(input)\n",
    "        # 循环遍历每个输出特征\n",
    "        for i in range(self.out_features):\n",
    "            # 使用线性操作处理该特征\n",
    "            output[:, i] = torch.nn.functional.linear(input, self.weights[i, :], self.biases[i])\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1398beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9c142a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4693, -1.2657, -0.5899],\n",
       "        [-0.0104, -0.6261, -2.3330],\n",
       "        [ 0.0194, -0.2384,  0.5886],\n",
       "        [-1.1524, -0.0638,  0.6801],\n",
       "        [-1.2305,  0.6501,  1.8660],\n",
       "        [-0.5634, -0.3469,  0.7891],\n",
       "        [-0.1487, -0.1093, -0.6186],\n",
       "        [ 0.4206, -1.8290,  0.5863],\n",
       "        [-0.4153,  0.2154, -1.4775],\n",
       "        [ 0.8700, -0.5762, -0.3122]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92942ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.4693],\n",
       "         [-0.0104],\n",
       "         [ 0.0194],\n",
       "         [-1.1524],\n",
       "         [-1.2305],\n",
       "         [-0.5634],\n",
       "         [-0.1487],\n",
       "         [ 0.4206],\n",
       "         [-0.4153],\n",
       "         [ 0.8700]]),\n",
       " tensor([[-1.2657],\n",
       "         [-0.6261],\n",
       "         [-0.2384],\n",
       "         [-0.0638],\n",
       "         [ 0.6501],\n",
       "         [-0.3469],\n",
       "         [-0.1093],\n",
       "         [-1.8290],\n",
       "         [ 0.2154],\n",
       "         [-0.5762]]),\n",
       " tensor([[-0.5899],\n",
       "         [-2.3330],\n",
       "         [ 0.5886],\n",
       "         [ 0.6801],\n",
       "         [ 1.8660],\n",
       "         [ 0.7891],\n",
       "         [-0.6186],\n",
       "         [ 0.5863],\n",
       "         [-1.4775],\n",
       "         [-0.3122]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.chunk(x, 4, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c382ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(len(torch.chunk(x, 4, dim = 1))):\n",
    "    a.append(torch.chunk(x, 4, dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18a8437e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(10/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce8f9ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a263a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
