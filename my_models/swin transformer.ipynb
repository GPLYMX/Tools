{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ac0d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Callable, Any, Tuple\n",
    "from timm.models.layers import to_2tuple,to_3tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afdf81cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" patch splitting \n",
    "\n",
    "    Args:\n",
    "        img_size (int): 输入图片的尺寸\n",
    "        patch_size (int): Patch token的尺度. Default: 4.\n",
    "        in_chans (int): 输入通道的数量\n",
    "        embed_dim (int): 线性投影后输出的维度\n",
    "        norm_layer (nn.Module, optional): 这可以进行设置，在本层中设置为了None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=512, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        # 先将img_size、patch_size转化为元组模式(224 , 224) 、 (4 , 4)\n",
    "        img_size = to_2tuple(img_size) \n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        # 计算出 Patch token在长宽方向上的数量\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        # 计算出patch的数量利用Patch token在长宽方向上的数量相乘的结果\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        # 判断是否使用norm_layer，在这里我们没有应用\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "    \t# 解析输入的维度\n",
    "        B, C, H, W = x.shape\n",
    "        # 判断图像是否与设定图像一致，如果不一致会报错\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        # 经过一个卷积层来进行一个线性变换，并且在第二个维度上进行一个压平操作，维度为(B, C, Ph*Pw),后在进行一个维与二维的一个转置，维度为：(B Ph*Pw C)\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  \n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1979745",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch合并.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): 输入特征的分辨率.\n",
    "        dim (int): 输入通道的数量\n",
    "        norm_layer (nn.Module, optional): 定义Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        # 通过一个线性层将4C降为2C\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        # 解析输入图像的分辨率，即输入图像的长宽\n",
    "        H, W = self.input_resolution\n",
    "        # 解析输入图像的维度\n",
    "        B, L, C = x.shape\n",
    "        # 判断L是否与H * W一致，如不一致会报错\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        # 判断输入图像的长宽是否可以被二整除，因为我们是通过2倍来进行下采样的\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "        # 将xreshape为维度：(B, H, W, C)\n",
    "        x = x.view(B, H, W, C)\n",
    "        # 切片操作，通过切片操作将将相邻的2*2的patch进行拼接\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        \n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        # 将合并好的patch通过c维进行拼接\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        # 将x的维度重置为B H/2*W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "        x = self.norm(x)\n",
    "        # 通过一个线性层进通道降维\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7804e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\" Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): 输入维度的数量\n",
    "        input_resolution (tuple[int]): 输入图像的分辨率\n",
    "        num_heads (int): 应用注意力头的数量\n",
    "        window_size (int): 窗口的尺寸\n",
    "        shift_size (int): SW-MSA的循环位移的大小，默认为零\n",
    "        mlp_ratio (float): mlp的隐层的比例,默认为零\n",
    "        qkv_bias (bool, optional): 是否应用位移偏量，具体实现在下文\n",
    "        qk_scale (float | None, optional): 如果设置，默认qk覆盖head_dim ** -0.5。\n",
    "        drop (float, optional): Dropout比例，防止过拟合的操作，默认为零\n",
    "        attn_drop (float, optional): AttentionDropout比例，默认为零\n",
    "        drop_path (float, optional): 随机深度比率，默认为0.0\n",
    "        act_layer (nn.Module, optional): 激活函数层，默认为GELU\n",
    "        norm_layer (nn.Module, optional): Normalization层， 默认为 nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # 如果Windows大小大于输入分辨率，则不分区Windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        # 如果Windows大小不符合指定的数值，则报错\n",
    "        assert 0 <= self.shift_size < self.window_size\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        # 窗口注意力，在接下来进行详细说明\n",
    "        self.attn = WindowAttention_acmix(\n",
    "                dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # 判断是否使用窗口移位，如果使用窗口移位，会使用mask windows，会在下文中进行分开讲解\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  \n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\t\t# 向模块添加一个attn_mask的持久缓冲区\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "    \t#解析输入图像的分辨率\n",
    "        H, W = self.input_resolution\n",
    "        # 解析输入的维度\n",
    "        B, L, C = x.shape\n",
    "        # 判断L和H * W是否一致\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\t\t# 进行原始的身份映射\n",
    "        shortcut = x\n",
    "        # 首先经过以一个LN层\n",
    "        x = self.norm1(x)\n",
    "        # 将维度重置为(B, H, W, C)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # 判断是否进行循环移位\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        shifted_x = self.attn(shifted_x, H, W, mask=self.attn_mask)\n",
    "\n",
    "        # 循环移位进行恢复\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbc788f2",
   "metadata": {},
   "outputs": [],
   "source": [
    " class WindowAttention(nn.Module):\n",
    "    \"\"\" 基于相对位置偏差的窗口多头自注意(W-MSA)模块。\n",
    "        它同时支持W-MSA和SW-MSA\n",
    "\n",
    "    Args:\n",
    "        dim (int): 输入通道的数量\n",
    "        window_size (tuple[int]): window的长和宽.\n",
    "        num_heads (int): attention heads的数量.\n",
    "        qkv_bias (bool, optional): 如果为True，则向query, key, value添加一个可学习的偏差。默认值: True\n",
    "        qk_scale (float | None, optional): 如果设置，覆盖head_dim ** -0.5的默认qk值\n",
    "        attn_drop (float, optional): attention weight丢弃率，默认: 0.0\n",
    "        proj_drop (float, optional): output的丢弃率. 默认:: 0.0\n",
    "    \"\"\"\n",
    "    # 实现W-MSA SW-MSA\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super(WindowAttention, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size ## Mh, Mw\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        # 定义相对位置偏差的参数表 (2*Mh-1 * 2*Mw-1, nH)\n",
    "        self.relative_positive_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads) \n",
    "        )\n",
    "        # 获取窗口内每个token的成对相对位置索引\n",
    "        # 第一行为feature map中每一个像素对应的行标（x）\n",
    "        # 第二行为feature map中每一个像素对应的列标（y）\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        # 拼接成横纵坐标\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w])) # [2, Mh, Mw]\n",
    "        # 沿着Mh这一维度进行展平\n",
    "        coords_flatten = torch.flatten(coords, 1) # [2, Mh*Mw] 绝对位置索引\n",
    "        # [2, Mh*Mw, 1] - [2, 1, Mh*Mw]\n",
    "        # [2, Mh*Mw, Mh*Mw] 得到相对位置索引的矩阵。 以每一个像素作为参考点 - 当前feature map/window当中所有的像素点绝对位置索引 = 得到相对位置索引的矩阵\n",
    "        # coords_flatten[:, :, None] 按w维度 每一行的元素复制\n",
    "        # coords_flatten[:, None, :] 按h维度 每一行元素整体复制\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        # permute： 将窗口中按每个像素求得的相对位置索引 组成矩阵\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous() # [Mh, Mw, 2]\n",
    "        # 二元索引->一元索引\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1) # [Mh*Mw, Mh*Mw]\n",
    "        # 放到模型缓存中\n",
    "        self.register_buffer('relative_position_index', relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        nn.init.trunc_normal_(self.relative_positive_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask: Optional[torch.Tensor] = None):\n",
    "        # 解析输入维度[batch_size * num_windows, Mh*Mw, total_embed_dim]\n",
    "        B_, N, C = x.shape\n",
    "        # qkv: -> [batch_size * num_windows, Mh*Mw, 3 * total_embed_dim]\n",
    "        # reshape: -> [batch_size * num_windows, Mh*Mw, 3, num_heads, embed_dim_per_head]\n",
    "        # permute: -> [3, batch_size * num_windows, num_heads, Mh*Mw, embed_dim_per_head]\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # [batch_size * num_windows, num_heads, Mh*Mw, embed_dim_per_head]\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        q = q * self.scale\n",
    "        # transpose: -> [batch_size * num_windows, num_heads,embed_dim_per_head, Mh*Mw]\n",
    "        # @: multiply: -> [batch_size * num_windows, num_heads, Mh*Mw, Mh*Mw]\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "        # self.relative_positive_bias_table.view: -> [Mh*Mw*Mh*Mw, num_head] -> [Mh*Mw, Mh*Mw, num_head]\n",
    "        relative_position_bias = self.relative_positive_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous() # [num_head, Mh*Mw, Mh*Mw]\n",
    "        # [batch_size * num_windows, num_heads, Mh * Mw, Mh * Mw]\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            # mask: [num_windows, Mh*Mw, Mh*Mw]\n",
    "            num_window = mask.shape[0]\n",
    "            # view: [batch_size, num_windows, num_heads, Mh * Mw, Mh * Mw]\n",
    "            # mask: [1, num_windows, 1, Mh*Mw, Mh*Mw]\n",
    "            attn = attn.view(B_ // num_window, num_window, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            # [batch_size*num_windows, num_heads, Mh * Mw, Mh * Mw]\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "        # @: [batch_size*num_windows, num_heads, Mh * Mw, embed_dim_per_head]\n",
    "        # transpose: [batch_size*num_windows, Mh * Mw, num_heads, embed_dim_per_head]\n",
    "        # reshape: [num_windows, Mh * Mw, num_heads*embed_dim_per_head]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce80c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定window大小，重新划分window\n",
    "def window_partition(x, window_size: int):\n",
    "    # 将feature map(image mask) 按照 window_size的大小 划分成一个个没有重叠的window\n",
    "    B, H, W, C = x.shape\n",
    "    # [B, H//M, MH, W//M, MW, C] MH: 为窗口H MW:为窗口W\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    # permute: [B, H//M, MH, W//M, MW, C] -> [B, H//M, W//M, MH, MW, C]\n",
    "    # contiguous(): 变为内存连续的数据\n",
    "    # view： [B, H//M, W//M, M, M, C] -> [B * window_num, MH, MW, C] \n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    # 最后会返回一个带有窗口数量以及窗口长宽的一个张量\n",
    "    return windows\n",
    "\n",
    "    if self.shift_size > 0:\n",
    "            # 判断是否使用SW-MSA，如果使用窗口移位，会使用mask windows\n",
    "            # 解析输入图像的分辨率\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            # 其中的切片操作划分后的结构如上图，切出每个窗口中分别具有相似元素的位置\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            # 开始切片，并且将相同的位置附上相同的数值            \n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "            # nW, window_size, window_size, 1\n",
    "            mask_windows = window_partition(img_mask, self.window_size) \n",
    "            # [B * window_num * C, MH*MW]\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            #  mask_windows.unsqueeze(1) 将每个窗口的行向量复制MH*MW次 \n",
    "            #  mask_windows.unsqueeze(2) 将每个窗口的行向量中每个元素 复制MH*MW次             \n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            # 同一区域为0 不同区域为非0数。 得到当前窗口中对应某一个像素 所采用的attention mask。\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "047589fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwimTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size=7, shift_size=0., mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super(SwimTransformerBlock, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim=dim, window_size=(self.window_size, self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = (int(dim * mlp_ratio))\n",
    "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        H, W = self.H, self.W # feature map H W\n",
    "        B, L, C = x.shape # L = H * W\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "        x_d = (self.window_size - H % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, 0, x_r, 0, x_d))\n",
    "        _, Hp, Wp, _ = x.shape # Hp Wp代表padding后的H W\n",
    "\n",
    "        if self.shift_size > 0.:\n",
    "            # SW-MSA 从上往下 从左往右\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)) # 上面的shift size移动到下面 左边移动右边\n",
    "        else:\n",
    "            # W-MSA\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "\n",
    "        # 特征图切成小窗口\n",
    "        x_windows = window_partition(shifted_x, self.window_size) # [B * window_num, MH, MW, C]\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C) # [B * window_num, MH*MW, C]\n",
    "\n",
    "        # W-MSA SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)\n",
    "\n",
    "        # 小窗口合并成特征图\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C) # [B * window_num, MH, MW, C]\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp) # [B, H, W, C]\n",
    "        # SW-MSA后还原数据 从下往上 从右往左\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        # 移除padding\n",
    "        if x_r > 0 or x_d > 0:\n",
    "            x = x[:, :H, :W, :].contiguous()\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c099d60b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'swin_transformer' from 'torchvision.models' (D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m swin_transformer\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'swin_transformer' from 'torchvision.models' (D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\models\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import swin_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f08b82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
